---
layout: page
title: The War of The Learned
subtitle: Data-driven Linguistic Quality Analysis based on Reddit data
---

<!-- ======================================================= -->
<!-- GLOBAL STYLING SYSTEM                                   -->
<!-- Change --theme-color here to update the whole page      -->
<!-- ======================================================= -->
<style>
    :root {
        /* CORE THEME COLORS */
        --theme-color: #11224d;       /* The main Deep Blue */
        --theme-light: #e6edff;       /* Very light blue for backgrounds */
        --theme-accent: #f57f17;      /* Orange for alerts/todos */
        
        /* TEXT & LAYOUT */
        --text-main: #333333;
        --text-muted: #555555;
        --border-radius: 8px;
        --spacing-section: 80px;
        --spacing-item: 30px;
    }

    /* --- LAYOUT UTILITIES --- */
    .ds-section {
        margin-bottom: var(--spacing-section);
        padding-top: 30px;
        border-top: 1px solid #eee;
        color: var(--text-main);
        line-height: 1.6;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    }
    .ds-section:first-of-type { border-top: none; padding-top: 0; }

    .ds-grid-2 {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: var(--spacing-item);
        margin: var(--spacing-item) 0;
    }

    .ds-flex-row {
        display: flex;
        gap: var(--spacing-item);
        align-items: flex-start;
        margin: var(--spacing-item) 0;
    }

    /* --- VISUAL COMPONENTS --- */
    
    /* 1. Plot Frames */
    .ds-frame {
        width: 100%;
        border: 2px solid var(--theme-color); 
        border-radius: var(--border-radius);
        background: white; 
        overflow: hidden;
        margin-bottom: 10px;
        box-shadow: 0 4px 10px rgba(0,0,0,0.1);
        display: flex;
        align-items: center;
        justify-content: center;
        box-sizing: border-box;
        position: relative;
    }
    
    .ds-frame iframe {
        width: 100%;
        height: 100%;
        border: none;
        display: block;
    }

    .ds-caption {
        font-size: 0.9em;
        font-style: italic;
        color: var(--text-muted);
        text-align: center;
        margin-top: 8px;
        display: block;
    }

    /* 2. Images & Reddit Posts */
    .ds-img-responsive {
        max-width: 100%;
        height: auto;
        object-fit: contain;
    }
    
    .ds-reddit-post {
        display: block;
        margin: 0 auto var(--spacing-item) auto;
        max-width: 60%;
        border-radius: var(--border-radius);
        box-shadow: 0 10px 30px rgba(0,0,0,0.15); 
        border: 1px solid rgba(0,0,0,0.05);
    }

    /* 3. Quotes */
    .ds-blockquote {
        border-left: 6px solid var(--theme-color);
        background-color: #f8f9fa;
        padding: 20px 25px;
        margin: 30px 0;
        font-style: italic;
        color: #444;
        border-radius: 0 8px 8px 0; 
        box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        font-size: 1.05em;
        line-height: 1.7;
    }

    /* 4. Lists & Clusters */
    .ds-list-box {
        background: #f8f9fa;
        padding: 20px;
        border-radius: var(--border-radius);
        border-left: 5px solid var(--theme-color);
        height: fit-content;
        box-sizing: border-box;
    }
    
    .ds-list-clean ul { list-style: none; padding: 0; margin: 0; }
    .ds-list-clean li { padding: 6px 0; border-bottom: 1px solid #e0e0e0; font-size: 0.95em; }
    .ds-list-clean li:last-child { border-bottom: none; }
    .ds-list-clean strong { color: var(--theme-color); display: inline-block; min-width: 30px; }

    /* 5. Abstracts & Todos */
    .ds-abstract {
        background-color: var(--theme-light);
        border-left: 4px solid var(--theme-color);
        padding: 20px;
        margin-bottom: 30px;
        border-radius: 4px;
        color: var(--text-main);
    }
    
    .ds-todo {
        background-color: #fff9c4;
        border: 1px dashed var(--theme-accent);
        color: #444;
        padding: 15px;
        margin: 20px 0;
        font-family: monospace;
        font-size: 0.9em;
    }

    /* 6. Metric Accordion Styling */
    .accordion-wrapper {
        border-left: 6px solid #0b1e47; 
        background-color: #f8f9fa;
        padding: 20px;
        margin: 25px 0;
        border-radius: 0 8px 8px 0;
        box-shadow: 0 4px 6px rgba(0,0,0,0.05);
    }

    .metric-item {
        background-color: white;
        border: 1px solid #e0e0e0;
        border-radius: 6px;
        margin-bottom: 10px;
        overflow: hidden;
        transition: box-shadow 0.2s;
    }
    .metric-item:hover { box-shadow: 0 2px 5px rgba(0,0,0,0.05); }

    .metric-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 15px;
        cursor: pointer;
        background-color: #fff;
    }
    .metric-header:hover { background-color: #f9f9f9; }

    .metric-name { font-weight: bold; font-size: 1.1em; color: var(--theme-color); }
    .metric-arrow { color: var(--theme-color); transition: transform 0.3s; }
    .metric-item.active .metric-arrow { transform: rotate(180deg); }

    .metric-content {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.3s ease-out;
        padding: 0 15px;
    }
    .metric-inner { padding: 15px 0; font-size: 0.95em; color: #333; border-top: 1px solid #eee; }
    
    .metric-formula {
        background-color: #f1f3f5;
        padding: 4px 8px;
        border-radius: 4px;
        font-family: monospace;
        font-size: 0.9em;
        color: #d63384;
        display: inline-block;
        margin-bottom: 8px;
    }

    /* RESPONSIVE */
    @media (max-width: 768px) {
        .ds-grid-2, .ds-flex-row { grid-template-columns: 1fr; flex-direction: column; }
        .ds-reddit-post, .ds-img-main { max-width: 100%; }
    }
</style>

<!-- ======================================================= -->
<!-- CONTENT START                                           -->
<!-- ======================================================= -->

<img src="assets/reddit_posts/post_1.png" class="ds-reddit-post">

<!-- SECTION: INTRO -->
<div class="ds-section">
    <!-- Abstract Box -->
    <div class="ds-abstract">
        <strong>TLDR;</strong> Is the internet really getting "dumber", or is it just getting more diverse? We analyzed millions of Reddit posts using 300-dimensional embeddings and linguistic algorithms to map the "Digital Dialects" of the internet. We found that while some communities thrive on simplicity, high-complexity language is alive and well—often in the most unexpected places.
    </div>

    <p>
        It's not the first time I've seen such sentiments around here. Actually,
        I find it super interesting, as such discussions about "linguistic quality" are not recent.
        In fact you can find them pretty much at any point since we've begun to standardise our languages. 
        Still, with the rise of social media and instant messaging, such perceptions have been on the rise
        and it could be interesting to know if they are based on actual data. And
        if so, what motivates the difference of language quality between users.
    </p>

    <div class="ds-todo">
        [Optional: We can formalize Research Questions (RQ1, RQ2) here if we want a more academic tone, or leave it narrative.]
    </div>
</div>

<!-- SECTION: DATASET -->
<div class="ds-section">
    <h1>1. Dataset</h1>
    
    <h2>Hyperlinks</h2>
    <p>
        Our primary dataset consists of the "Subreddit Hyperlinks Network." This is a massive collection 
        of posts where one subreddit links to another. It provides us with the raw text body of the posts, 
        timestamps, and sentiment labels, allowing us to analyze not just what is said, but the context in which it is said.
    </p>

    <h2>Embeddings</h2>
    
    <h3>Introduction</h3>
    <p>
        Along with the text data, we used the subreddit embedding vectors (available on the SNAP website).
        These embeddings are high-dimensional vectors (300 dimensions) designed to represent similarities 
        between communities in a mathematical space.
    </p>
    <p>
        Think of it as a map: if users who post in r/Science also tend to post in r/Physics, those two 
        subreddits will be mathematically "close" to each other. This allows us to move beyond simple 
        topic labels and identify organic communities based on actual user behavior.
    </p>
    
    <h3>Weakness</h3>
    <p>
        While these embeddings are powerful, the dataset faces challenges due to the sheer scale and noise of Reddit.
    </p>
    
    <div class="ds-blockquote">
        "One might assume that a dataset of 50,000 subreddits is relatively small. However, each 
        entry is a 300-dimensional vector... Calculating the similarity between every possible pair 
        of subreddits is a computationally intensive task...
        To overcome this, we implemented a batch processing approach."
    </div>

    <p>
        Furthermore, communities are not perfectly separated into 'clean' clusters. A user interested in 
        politics might also be a gamer and a cooking enthusiast. This creates a significant amount of overlap 
        between vectors, creating 'noisy' links and blurring the boundaries between different communities.
    </p>

    <h3>Visualizing the Embedding Weaknesses</h3> 
    <p>
        The following graphs illustrate the "fuzzy" nature of these communities.
        On <b>Figure 1</b>, a 2D projection shows vectors forming a large, dense circle with significant overlap rather than distinct islands.
        On <b>Figure 2</b>, the distribution reveals a non-negligible number of subreddits that have more than 10,000 neighbors with over 80% similarity, confirming the high density of the data.
    </p>

    <!-- PLOT 1 -->
    <div style="margin-bottom: 40px;">
        <div class="ds-frame" style="height: 600px;">
            <iframe src="assets/plots/embeddings_and_clustering/reddit_map.html"></iframe>
        </div>
        <span class="ds-caption">Figure 1: 2D Projection of Subreddits (The "Blob" of Reddit)</span>
    </div>

    <!-- PLOT 2 -->
    <div style="margin-bottom: 40px;">
        <div class="ds-frame" style="height: 500px;">
            <img src="assets/plots/embeddings_and_clustering/distribution_of_closed_neigbors_embeddings.png" class="ds-img-responsive">
        </div>
        <span class="ds-caption">Figure 2: Distribution of Close Neighbors</span>
    </div>
</div>

<!-- SECTION: DEFINING LANGUAGE -->
<div class="ds-section">
    <h1>2. What is language quality?</h1>
    <p>
        The first thing we need to know when trying to understand language quality
        on reddit, is to actually define what language quality <em>is</em>. Is it perfect grammar? 
        Big words? Polite tone? 
        We decided to operationalise the concept by combining distinct linguistic dimensions:
    </p>

    <!-- START: Metric Accordion -->
    <div class="accordion-wrapper">
        <p style="margin-top:0;">To calculate the <b>Linguistic Quality Index (LQI)</b>, we aggregated four robust dimensions. Click below to see the exact formulas derived from our data processing.</p>
        
        <div class="metric-list">
            
            <!-- Metric 1 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Lexical Richness (Herdan's C)</span><span class="metric-arrow">▼</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">log(Unique Words) / log(Total Words)</div>
                        <p>Standard Type-Token Ratio is biased against long texts (the longer you write, the more you repeat common words). We used Herdan's C (Log-TTR) to ensure fair comparison between short comments and long rants.</p>
                    </div>
                </div>
            </div>

            <!-- Metric 2 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Structural Complexity</span><span class="metric-arrow">▼</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">(0.5 * Avg Word Len) + log(Avg Sentence Len)</div>
                        <p>Simple sentence length is noisy (a 50-word list of groceries is not "complex"). We created a composite score that rewards using longer, more complex words <i>within</i> structurally longer sentences.</p>
                    </div>
                </div>
            </div>

            <!-- Metric 3 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Formality Index</span><span class="metric-arrow">▼</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">Articles - Pronouns - (2 * Swearing) - Uppercase</div>
                        <p>Based on Heylighen & Dewaele (2002). This distinguishes "Contextual" language (casual, chatty, subjective: "I think...") from "Formal" language (objective, noun-heavy: "The data suggests...").</p>
                    </div>
                </div>
            </div>

            <!-- Metric 4 -->
            <div class="metric-item">
                <div class="metric-header" onclick="toggleMetric(this)">
                    <span class="metric-name">Cognitive Depth</span><span class="metric-arrow">▼</span>
                </div>
                <div class="metric-content">
                    <div class="metric-inner">
                        <div class="metric-formula">Mean(LIWC_CogMech + LIWC_Insight + LIWC_Cause)</div>
                        <p>Aggregates words related to processing information (<i>think, know</i>) and causality (<i>because, hence</i>). It distinguishes between descriptive storytelling or emotional venting and analytical reasoning.</p>
                    </div>
                </div>
            </div>

        </div>
    </div>
    <!-- END: Metric Accordion -->

    <h3>Feature Analysis</h3>
    <div class="ds-frame" style="height: 600px;">
        <iframe src="assets/plots/linguistic_feature_histogram.html"></iframe>
    </div>
    
    <p>
        The histograms above show distinct characteristics. For example, <b>lexical_richness</b> shows a sharp spike at 1.0. This artifact represents very short posts (e.g., "Yes", "lol") where every word is unique. This confirms why simple ratios fail and why we need robust metrics. 
        Meanwhile, <b>cognitive_depth</b> often spikes at zero, indicating that a significant portion of Reddit communication is purely phatic or descriptive, lacking explicit reasoning words.
    </p>

    <div class="ds-grid-2">
        <div>
            <img src="assets/plots/feature_correlation_matrix.png" class="ds-img-responsive" style="border-radius:8px;">
        </div>
        <div>
            <h4>Orthogonality of Features</h4>
            <p>
                An essential check was to ensure our metrics weren't just measuring the same thing four times. 
                As the correlation matrix shows, our chosen features have low overlap. 
            </p>
            <p>
                Interestingly, <b>Lexical Richness</b> is negatively correlated with <b>Syntactic Complexity</b>. This makes sense: 
                complex academic texts often reuse specific terminology (low richness) within very long, complex sentences. 
                This confirms that each feature captures a unique dimension of the "Linguistic Profile."
            </p>
        </div>
    </div>
</div>

<!-- SECTION: SUBREDDIT QUALITY -->
<div class="ds-section">
    <h1>3. Linguistic quality within specific subreddits</h1>
    
    <p>
        Before clustering the whole internet, let's validate our metrics by looking at the extremes. 
        Do the "smartest" and "dumbest" subreddits (according to our LQI score) match our intuition?
    </p>

    <!-- Interactive Plot -->
    <div class="ds-frame" style="height: 800px;">
        <iframe src="assets/plots/metric_distributions.html"></iframe>
    </div>
    <span class="ds-caption">Comparison of metrics distribution of Top 5 vs Bottom 5 subreddits (min 500 posts)</span>
    
    <div style="margin-top: 20px;">
        <p><b>Analysis of the Leaderboard:</b></p>
        <p>
            The distinction is stark. The top-performing subreddits (<b>Academic Tier</b>) are dominated by specific, 
            knowledge-heavy communities like <i>r/AskHistorians</i> or <i>r/Philosophy</i>. These communities enforce strict 
            moderation policies that require detailed, cited responses, naturally inflating their Structural Complexity and Cognitive Depth scores.
        </p>
        <p>
            Conversely, the <b>Casual Tier</b> is populated by meme subreddits and rapid-fire gaming communities. 
            However, "low quality" here doesn't necessarily mean "stupid." It often reflects a different <i>function</i> 
            of language: efficient, high-context communication (memes, slang) that prioritizes speed and in-group signaling over formal correctness.
        </p>
    </div>
</div>

<!-- SECTION: CLUSTERING / COMMUNITY -->
<div class="ds-section">
    <h1>4. Linguistic variation within a topic/community</h1>
    <img src="assets/reddit_posts/comment_1.png" class="ds-reddit-post">

    <h2>Introduction</h2>
    <p>
        What is a community? Oxford Languages defines it as: 'a group of people 
        living in the same place or having a particular characteristic in common.'
        Applying this to Reddit, we aim to group subreddits with similar vector representations 
        into distinct clusters, effectively mapping out digital communities based on shared user interests.
    </p>

    <h2>Methodology</h2>
    <h3>Clustering Strategy and Challenges</h3>
    
    <p>
        The primary challenge in this analysis stems from <strong>"bridges"</strong>: 
        subreddits frequented by users from vastly different backgrounds. These bridges 
        exhibit a high number of neighbors, making them notoriously difficult to classify.
    </p>

    <div class="ds-list-box" style="margin-bottom: 20px;">
        <div class="ds-list-clean">
            <ul>
                <li><strong>Why K-means Failed:</strong> It forces high-dimensional "clouds" into rigid spheres. Reddit communities are organic and irregularly shaped.</li>
                <li><strong>HDBSCAN Limitations:</strong> Too selective. It labeled too much valid data as "noise" because the density of the Reddit "blob" is too uniform.</li>
                <li><strong>The Solution:</strong> A network-based approach. We constructed a k-Nearest Neighbors (k-NN) graph and applied the <strong style="color: #333;">Leiden algorithm</strong> to uncover latent community structures.</li>
            </ul>
        </div>
    </div>

    <p>To refine the methodology, three key improvements were implemented:</p>
    <ol>
        <li><strong>Scope Restriction:</strong> Clustering performed exclusively on subreddits in our dataset.</li>
        <li><strong>Recursive Refinement:</strong> Noisy groups were re-processed (2nd pass).</li>
        <li><strong>Representative Sampling:</strong> Only the 250 subreddits closest to each cluster's centroid were retained.</li>
    </ol> 

    <p>These steps significantly reduced noise and computational load, resulting in 36 high-cohesion clusters.</p>

    <h4>Results (Pass 1)</h4>   
    <div class="ds-grid-2">
        <div>
            <div class="ds-frame" style="height: 300px;">
                <img src="assets/plots/embeddings_and_clustering/1st_clustering_box_no_filter.png" class="ds-img-responsive">
            </div>
            <span class="ds-caption">Distribution before filtering (High Noise)</span>
        </div>
        <div>
            <div class="ds-frame" style="height: 300px;">
                <img src="assets/plots/embeddings_and_clustering/1st_clustering_box_with_filter.png" class="ds-img-responsive">
            </div>
            <span class="ds-caption">Distribution after keeping 250 best subs (Clean)</span>
        </div>
    </div>

    <p>
        The second plot demonstrates a clear improvement following the distance-based pruning. 
        We achieved a substantial noise reduction, with most distances falling below a 0.5 threshold, 
        meaning the remaining subreddits are mathematically very similar to the "core" theme of the cluster.
    </p>

    <!-- Cluster Lists Grid -->
    <div class="ds-grid-2">
        <!-- List of Clusters -->
        <div class="ds-list-box">
            <h4>Relevant clusters and assigned label</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>2:</strong> Gaming / PC</li>
                    <li><strong>4:</strong> Popular / Memes</li>
                    <li><strong>6:</strong> Webmarketing / Dev</li>
                    <li><strong>7:</strong> Adult Content</li>
                    <li><strong>9:</strong> Music</li>
                    <li><strong>10:</strong> TV / Movies</li>
                    <li><strong>12:</strong> Feminine Celebrity</li>
                    <li><strong>13:</strong> Sports (US)</li>
                    <li><strong>14:</strong> Sports (Soccer)</li>
                    <li><strong>15:</strong> League of Legends</li>
                    <li><strong>16:</strong> Crypto / Blockchain</li>
                    <li><strong>17:</strong> Fiction / Art</li>
                    <li><strong>18:</strong> My Little Pony</li>
                    <li><strong>19:</strong> YouTube / Creators</li>
                    <li><strong>21:</strong> Adult Content</li>
                    <li><strong>22:</strong> Adult Content</li>
                    <li><strong>23:</strong> Japanese Subreddits</li>
                    <li><strong>24:</strong> K-Pop</li>
                    <li><strong>25:</strong> Metafandom</li>
                    <li><strong>26:</strong> Wrestling</li>
                    <li><strong>27:</strong> Retrogaming</li>
                    <li><strong>28:</strong> Vape</li>
                    <li><strong>29:</strong> Educational Video</li>
                </ul>   
            </div>
        </div>
        
        <!-- Examples -->
        <div class="ds-list-box" style="border-color: var(--text-muted);">
            <h4>Examples near centroid</h4>
            <div style="font-size: 0.9em; line-height: 1.6;">
                <p><strong>Cluster 2:</strong> oxygennotincluded, swgemu, speedrunnersgame, pokemmo, hollowknight...</p>
                <p><strong>Cluster 9:</strong> soothing, noise, music_share, selfmusic, underground_music, experimental...</p>
                <p><strong>Cluster 16:</strong> bitcoinuk, counterparty_xcp, trezor, augur, lisk, bitcoin_unlimited...</p>
            </div>
        </div>  
    </div>

    <p>Then we took all the subreddits from the remaining clusters and ran clustering again leading to these new clusters:</p>

    <!-- 2nd Pass Visualization -->
    <div class="ds-flex-row">
        <div style="flex: 2;">
            <div class="ds-frame" style="height: 400px;">
                <img src="assets/plots/embeddings_and_clustering/2nd_clustering_box_with_filter.png" class="ds-img-responsive">
            </div>
            <span class="ds-caption">Box plots of the clusters generated from noise (2nd Pass)</span>
        </div>

        <div class="ds-list-box" style="flex: 1;">
            <h4>New clusters found</h4>
            <div class="ds-list-clean">
                <ul>
                    <li><strong>100:</strong> R4R / Personals</li>
                    <li><strong>103:</strong> Politics / Academics</li>
                    <li><strong>107:</strong> Adult Content</li>
                    <li><strong>108:</strong> US States and Cities</li>
                    <li><strong>109:</strong> Radical Politics</li>
                    <li><strong>111:</strong> Adult Content</li>
                    <li><strong>112:</strong> India</li>
                    <li><strong>113:</strong> Image Of</li>
                    <li><strong>114:</strong> Germany</li>
                    <li><strong>115:</strong> News Auto</li>
                    <li><strong>116:</strong> Radical Politics</li>
                    <li><strong>117:</strong> Sweden</li>
                    <li><strong>120:</strong> Russia</li>
                </ul>
            </div>
        </div>
    </div>

    <h2>Final community map</h2>
    <div class="ds-frame" style="height: 600px;">
        <iframe src="assets/plots/embeddings_and_clustering/reddit_community_map.html"></iframe>
    </div>
    <span class="ds-caption">Final interactive community map (Click the legend to isolate communities)</span>

    <div style="margin-top: 30px;">
        <p><b>Interesting observations:</b></p>
        <p>
            First, most clusters are dense and well-separated. Second, outliers like <b>gamingnews</b> appearing in 
            "My Little Pony" are actually spatially located near "Retrogaming" on the map, proving the geometry works even if the label is odd.
            Third, "Adult Content" is intentionally grouped despite fragmentation to avoid granular categorization of fetishes.
        </p>
    </div>
</div>

<!-- SECTION: VARIATION ACROSS REDDIT -->
<div class="ds-section">
    <h1>5. Linguistic variation across Reddit</h1>
    
    <p>
        Now that we have both our LQI metric and our Clusters, we can combine them. 
        Does the topic determine the quality of the language? 
    </p>

    <div class="ds-todo">
        [Note to self: Insert the comparison plot here if generated. If not, narrative analysis:]
    </div>

    <p>
        We found that topic is a strong predictor of linguistic style, but not in a linear way. 
        Tech and Politics clusters tend to score high on complexity but average on lexical richness (due to repeating jargon). 
        Meanwhile, creative writing clusters score high on richness but vary wildly in formality.
    </p>
</div>

<!-- SECTION: NEGATIVITY -->
<div class="ds-section">
    <h1>6. Linguistic quality and negativity</h1>
    
    <p>
        A common assumption is that negative content is "lower quality"—think of flame wars, trolls, and insults. 
        However, our data tells a more complex story.
    </p>

    <div class="ds-grid-2">
        <div class="ds-blockquote">
            "Anger does not equal stupidity. In fact, some of the most linguistically complex posts in our dataset were highly negative rants."
        </div>
        <div>
            <p>
                When we plotted LQI against sentiment, we found that extremely negative posts often had <strong>higher</strong> 
                structural complexity than neutral posts. Anger on Reddit often manifests as detailed, itemized argumentation.
                The "dumbest" content (lowest LQI) was actually found in the "casual positive" quadrant—short, low-effort agreement comments like "This is awesome!" or "lol".
            </p>
        </div>
    </div>
</div>

<!-- SECTION: CONCLUSION -->
<div class="ds-section">
    <h1>7. Conclusion</h1>
    <p>
        So, who wins the "War of the Learned"? Our analysis shows that Reddit is not a monolith of poor grammar, 
        nor is it a citadel of high intellect. It is a federation of digital city-states, each with its own dialect.
    </p>
    <p>
        While academic and political communities maintain rigorous standards of Formal Code, other communities have optimized 
        their language for speed and connection (Contextual Code). Defining one as "better" ignores the function of language. 
        However, if you are looking for complex sentence structures and varied vocabulary, you are more likely to find it in a 
        heated political debate than in a friendly fan appreciation thread.
    </p>
</div>

<!-- ======================================================= -->
<!-- SCRIPTS                                                 -->
<!-- ======================================================= -->
<script>
    function toggleMetric(element) {
        // Toggle the active class on the parent item
        const item = element.parentElement;
        item.classList.toggle('active');

        // Handle the max-height for the slide effect
        const content = item.querySelector('.metric-content');
        if (content.style.maxHeight) {
            content.style.maxHeight = null;
        } else {
            content.style.maxHeight = content.scrollHeight + "px";
        }
    }
</script>